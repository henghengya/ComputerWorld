---
title: 03：梯度下降和反向传播
date: 2021-08-31 23:00
tags:
  - 
alias: 
category: 
description: 
from: 
url: 
references: 
---
# 03：梯度下降和反向传播


## 梯度下降

上节我们将误差e与斜率w之间的关系总结为一个一元二次函数，并且可以通过直接找这个一元二次函数的最低点，一步到位找到最佳的w值使得e最小。但是这种方式在输入特征过多、样本数量过大时，非常消耗计算资源。因此，我们可以通过“以时间换空间”的方式，慢慢找到这个最低点。

那么如何判断该往哪里挪呢，一次挪多大呢？我们知道，一个开口向上的抛物线的最低点斜率是0，在最低点左侧斜率是负数，右侧斜率是正数。因此我们只要知道代价函数曲线在当前的w值下的斜率是正负，就可以判断w应该调大还是调小了。

### 求一个一元二次函数的斜率

第一种方式：取两个足够近的点，计算得到。设$w_1=w$ $w_2=w+\Delta w$ 那么我们可以得到这两个点之间的斜率：
$$ \frac{e_1 - e_2}{w_1 - w_2}
= \frac{aw^2+bw+c - (a(w + \Delta w)^2+b(w+\Delta w)+c}{w - (w + \Delta w)}
= 2aw+a\Delta w+b $$

而其中的Δw取的是一个无穷小，用数学公式表述就是极限：
$$\lim_{\Delta x \to 0} 2aw+a\Delta w+b
= 2aw + b$$

而这种求得的就是一个函数的导数，这种求导的方法就是定义法。原则上用定义法可以求解出任意一个函数的导数（函数可导时），但是略有繁琐。而常用的基本函数就那么多，那么就可以通过另一种方法，就是求导公式和求导法则。而根据求导公式和法则得到的导数公式也得到了：
$$2aw + b$$

### 调整一次的幅度

经前面计算得到了导数，那么神经元就可以通过导数的正负调节w的大小了。那么一次调节多少合适呢？

当w距离最低点较远的时候，我们期望w能够调整的大一些；而当w距离最低点很近了，那么就期望w调整幅度小一些。而我们可以看出，当w离的越远，斜率的绝对值越大；当w离的越近，斜率的绝对值越小，接近于0。而w在最低点左右两侧的正负号也不同。正好可以通过w-斜率得到新的w，同时给斜率乘一个较小的学习率ɑ。

这种通过不同点的斜率调整w的方式，就是所谓的`梯度下降`。梯度是一个比斜率更加广泛的概念，当函数不是一条线，而是一个面或更高维度时，斜率对应的就是梯度。