---
title: 02：方差代价函数
date: 2021-08-14 20:45:38
tags:
  - Python
  - 最小二乘法
alias: 
category: 
stars: 
from: 
url: 
references: 
 - https://www.bilibili.com/cheese/play/ep6588
---
# 02：方差代价函数

## 现代神经网络

### 如何评估误差

上节中，通过“标准值-预测值=误差”的方式计算求得的值，由于带有正负，并不能实际的表示两个值的差距。所以可以经过绝对值处理。这就是`绝对差`。但绝对值在数学处理和计算机编码上都不方便，所以使用替代的方式：将差值取平方，正负号就消失了。这就是`平方误差`。

因此，预测值与标准值之间的平方误差越小，说明偏离事实越小。而对一组数据来说，将平方误差相加再求平均值，就是这组数据的`整体误差`

### 误差与w之间的关系

由于是因为w的不同导致了误差的不同，那么误差与w之间有什么联系呢？

取一个豆豆来看，大小为x，毒性为y。那么其平方误差是怎么表示呢？

$$ e = (y - w * x)^2 = y^2 + x^2 * w^2 + y * x * w $$

由于xy都是确定的，所以对每一个豆豆来说，误差其实就是一个w的一元二次函数。同理，在一组豆豆上计算出来的整体误差也是一个一元二次函数。用数学公式表示为：
$$e=\frac{1}{m}\sum^{m}_{i=0}(x_i^2*w^2+(-2x_iy_i)*w+y_i^2)$$

### 最小二乘法

对于一个质地不均匀的硬币，如何判断抛掷一次正反面出现的概率呢？最简单直接的方式就是不断的抛掷。这就是：`事物出现的频率必定收敛于它的概率`。

而对与我们通过大量的数据来判断评估w到底合不合适，这实际上是数理统计学里的一个重要分支——`回归分析`。而评估的标准是`均方误差`，并试图让其最小，即回归分析中的最小二乘法。此时，我们将w作为自变量，误差e作为因变量，形成的新的函数，就是`代价函数`。而利用代价函数的最低点的w值，将其放回到预测函数中，就形成了对数据的最佳拟合。

### 如何得到最低点的w值

对于上面的一元二次函数的代价函数，其最低点可以直接通过抛物线的顶点坐标公式计算出来。
$$
w=-\frac{b}{2a}
=-\frac{\frac{1}{m}\sum^m_{i=0}(-2x_iy_i)}{2\frac{1}{m}\sum^m_{i=0}x_i^2}
=-\frac{\sum^m_{i=0}(x_iy_i)}{\sum^m_{i=0}x_i^2}
$$

这种一次性求解出让误差最小的w取值的方法即`正规方程`。只不过完整的正规方程中使用矩阵或向量作为原数据。这种一步到位的方式只适用于样本数量比较少的时候，对于海量的数据就太消耗资源了。

## 编程实验

```python
import dataset
import numpy as np
import matplotlib.pyplot as plt

counts = 100
xs, ys = dataset.get_beans(counts)

# 计算在不同的w取值下，误差的大小
ws = np.arange(0, 3, 0.1)
es = []
for w in ws:
    y_pre = w * xs
    e = (1 / counts) * (np.sum((ys - y_pre)**2))
    es.append(e)

plt.plot(ws, es)
plt.show()

# 计算最小值，并代入预测函数中画图
w_best = np.sum(xs * ys) / np.sum(xs**2)
plt.scatter(xs, ys)
y_pre = w_best * xs
plt.plot(xs, y_pre)
plt.show()
```